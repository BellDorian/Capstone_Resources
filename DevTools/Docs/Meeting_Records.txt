

Notes from meeting (9/14/23, 7pm - 8pm):

-Videos denoted by shot angle (topside, side1 or side2)
  -within each angle, the videos are split into equal parts with equal number of frames (~1020frames per video(5) per angle(3). 15,300 frames total)
  -Example: Videos/side2_1, Videos/side2_5
  -Team member assignments:
            Adam: Set 1 (all views)
            Avni: Set 2 (all views)
            Dorian: Set 3 (all views)
            Kelly: Set 4 (all views)
            Tristan: Set 5 (all views)
  
-Ground Truthing
  -use package in MatLab: Automated Driving Toolbox
  -frame-by-frame manually ground truth Face, Eyes, Mouth, and Hands(x2) 
  -Color schema:
        Face: Red -- [0.00 1.00 0.00] Red (r)
        Eyes: Green -- [0.00 1.00 0.00] Green (g)
        Mouth: Blue -- [0/00 0.45 0.74] Blue (b)
        Hands: Purple -- [1.00 0.00 1.00] Magenta (m)
        
-Next Meeting 
  -Thursday Sept 21, 7pm
  -Have at least 1 set of frames done ground truthing
  
- I have been busy researching yolo. I created a place above where you can go in and click on the links. KK

- I decided to train the detectors using Faster R CNN rather than yolo.

		I made a script that extracted each frame from the video that is used for ground truthing. Then we stored the video path
	 with the corresponding bounding box for each frame within a table. The data was stored so that the Face detector was
	 only trained using Face, Eyes, and Mouth and that there was a separate detector for Hands. That table was then used 
	 to train R CNN using the trainRCNNObjectDetector. The data is save and exported to excel file using another script. TK 

	10/15/2023:
			the script ran well with 2 epochs but that was the only way we had enough time to train the data by the due date. 
		we will be training the rest of data, especially for topside, beginning immedietly.
